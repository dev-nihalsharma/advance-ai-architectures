{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom math import ceil","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-14T12:08:50.340262Z","iopub.execute_input":"2024-06-14T12:08:50.340670Z","iopub.status.idle":"2024-06-14T12:08:53.883409Z","shell.execute_reply.started":"2024-06-14T12:08:50.340637Z","shell.execute_reply":"2024-06-14T12:08:53.882196Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"EfficientNet is the best imageproccessing model out there:\nhttps://www.youtube.com/watch?v=qoSKbMbf1Pw&t=26s","metadata":{}},{"cell_type":"markdown","source":"After Research:\naplha = 1.2; beta = 1.1; gamma = 1.15;  \n\n* there are only 3 ways we can improve models accuracy. efficient net is a model that manages to perform all three of them \n1. width scalling(increasing channels -> inc. width) \n2. depth scalling(number of layers) \n3. resolution scalling(inc. res of image(self.exapnd) then dec it back to same res(self.se) after putting hidden layers in it)","metadata":{}},{"cell_type":"code","source":"# Architechture Config\n\nbase_model = [\n    # expand_ratio, channels, repeats, stride, kernel_size\n    [1, 16, 1, 1, 3],\n    [6, 24, 2, 2, 3],\n    [6, 40, 2, 2, 5],\n    [6, 80, 3, 2, 3],\n    [6, 112, 3, 1, 5],\n    [6, 192, 4, 2, 5],\n    [6, 320, 1, 1, 3],\n]\n\nphi_values = {\n    # tuple of: (phi_value, resolution, drop_rate)\n    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n    \"b1\": (0.5, 240, 0.2),\n    \"b2\": (1, 260, 0.3),\n    \"b3\": (2, 300, 0.3),\n    \"b4\": (3, 380, 0.4),\n    \"b5\": (4, 456, 0.4),\n    \"b6\": (5, 528, 0.5),\n    \"b7\": (6, 600, 0.5),\n}\n","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:03:39.528217Z","iopub.execute_input":"2024-06-12T19:03:39.528670Z","iopub.status.idle":"2024-06-12T19:03:39.537057Z","shell.execute_reply.started":"2024-06-12T19:03:39.528634Z","shell.execute_reply":"2024-06-12T19:03:39.535796Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Util Blocks\nclass CNNBlock(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride, padding, groups=1\n    ):\n        super(CNNBlock, self).__init__()\n        self.cnn = nn.Conv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            groups=groups,\n            bias=False,\n        )\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.silu = nn.SiLU()  # SiLU <-> Swish\n\n    def forward(self, x):\n        return self.silu(self.bn(self.cnn(x)))\n\nclass SqueezeExcitation(nn.Module):\n    def __init__(self, in_channels, reduced_dim):\n        super(SqueezeExcitation, self).__init__()\n        self.se = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),  # C x H x W -> C x 1 x 1\n            nn.Conv2d(in_channels, reduced_dim, 1),\n            nn.SiLU(),\n            nn.Conv2d(reduced_dim, in_channels, 1),\n            nn.Sigmoid(),\n        )\n\n    \n    def forward(self, x):\n        return x * self.se(x) # each channel(x) * priority of channel\n\nclass InvertedResidualBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        expand_ratio,\n        reduction=4,  # squeeze excitation\n        survival_prob=0.8,  # for stochastic depth\n    ):\n        super(InvertedResidualBlock, self).__init__()\n        self.survival_prob = 0.8\n        self.use_residual = in_channels == out_channels and stride == 1\n        hidden_dim = in_channels * expand_ratio\n        self.expand = in_channels != hidden_dim\n        reduced_dim = int(in_channels / reduction)\n\n        if self.expand:\n            self.expand_conv = CNNBlock(\n                in_channels,\n                hidden_dim,\n                kernel_size=3,\n                stride=1,\n                padding=1,\n            )\n\n        self.conv = nn.Sequential(\n            CNNBlock(\n                hidden_dim,\n                hidden_dim,\n                kernel_size,\n                stride,\n                padding,\n                groups=hidden_dim,\n            ),\n            SqueezeExcitation(hidden_dim, reduced_dim),\n            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n        )\n\n    def stochastic_depth(self, x):\n        if not self.training:\n            return x\n\n        binary_tensor = (\n            torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n        )\n        return torch.div(x, self.survival_prob) * binary_tensor\n\n    def forward(self, inputs):\n        x = self.expand_conv(inputs) if self.expand else inputs\n\n        if self.use_residual:\n            return self.stochastic_depth(self.conv(x)) + inputs\n        else:\n            return self.conv(x)\n    ","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:03:40.198120Z","iopub.execute_input":"2024-06-12T19:03:40.198566Z","iopub.status.idle":"2024-06-12T19:03:40.215597Z","shell.execute_reply.started":"2024-06-12T19:03:40.198532Z","shell.execute_reply":"2024-06-12T19:03:40.214107Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"class EfficientNet(nn.Module):\n    def __init__(self, version, num_classes):\n        super(EfficientNet, self).__init__()\n        width_factor, depth_factor, drop_rate = self.calculate_factors(version)\n        last_channels = ceil(1280*width_factor)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.features = self.create_features(width_factor, depth_factor, last_channels)\n        self.classfier = nn.Sequential(\n        nn.Dropout(drop_rate),\n        nn.Linear(last_channels,num_classes)\n        )\n        \n    def calculate_factors(self,version, alpha=1.2, beta=1.1):\n        phi, res, drop_rate = phi_values[version]\n        \n        depth_factor = alpha**phi\n        width_factor = beta**phi\n        \n        return width_factor, depth_factor, drop_rate\n    \n    def create_features(self, width_factor, depth_factor, last_channels):\n        channels = int(32 * width_factor)\n        features = [CNNBlock(3, channels, stride=2, kernel_size=3, padding=1)]\n        in_channels = channels\n        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n            out_channels = 4*ceil(int(channels*width_factor) / 4)\n            layers_repeats = ceil(repeats*depth_factor)\n            \n            for layer in range(layers_repeats):\n                features.append(InvertedResidualBlock(in_channels, out_channels,expand_ratio=expand_ratio, kernel_size=kernel_size, stride=stride if layer == 0 else 1, padding=kernel_size//2))\n                in_channels = out_channels\n                \n        features.append(CNNBlock(in_channels, last_channels, stride=1, kernel_size=1,padding=0))\n        \n        return nn.Sequential(*features)\n    \n    def forward(self, x):\n        x = self.pool(self.features(x))\n        return self.classfier(x.view(x.shape[0], -1))","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:07:43.613307Z","iopub.execute_input":"2024-06-12T19:07:43.613762Z","iopub.status.idle":"2024-06-12T19:07:43.628087Z","shell.execute_reply.started":"2024-06-12T19:07:43.613725Z","shell.execute_reply":"2024-06-12T19:07:43.626754Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nversion = 'b0'\nnum_examples, num_classes = 4, 10\nphi, res, drop_rate = phi_values[version]\n\nmodel = EfficientNet(version, num_classes).to(device)\nx = torch.randn(4, 3, res, res).to(device)\nprint(model(x).shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T19:07:44.054158Z","iopub.execute_input":"2024-06-12T19:07:44.054606Z","iopub.status.idle":"2024-06-12T19:07:44.522376Z","shell.execute_reply.started":"2024-06-12T19:07:44.054570Z","shell.execute_reply":"2024-06-12T19:07:44.521219Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"torch.Size([4, 10])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}